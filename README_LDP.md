# Lake Flow Declarative Pipelines (LDP)

## üìñ Vis√£o Geral

O **Lake Flow Declarative Pipelines (LDP)** (anteriormente conhecido como *Delta Live Tables* ou DLT) √© uma estrutura de ETL baseada no Apache Spark projetada para criar pipelines de dados confi√°veis e de f√°cil manuten√ß√£o.

Diferente da abordagem imperativa padr√£o do Spark, o LDP utiliza um modelo **declarativo**: voc√™ define o resultado desejado das transforma√ß√µes (o "o qu√™") e o framework gerencia a complexidade da execu√ß√£o, orquestra√ß√£o e infraestrutura (o "como").

> **Nota:** A Databricks abriu o c√≥digo-fonte desta solu√ß√£o, integrando-a ao ecossistema Apache Spark sob o nome **Spark Declarative Pipelines**.

---

## üöÄ Principais Benef√≠cios

* **Abstra√ß√£o de Complexidade:** Elimina a necessidade de gerenciar detalhes de baixo n√≠vel do Spark (checkpoints, escritas de stream).
* **Orquestra√ß√£o Autom√°tica:** Identifica depend√™ncias entre tabelas e visualiza o fluxo de execu√ß√£o (DAG) automaticamente.
* **Gerenciamento Aut√¥nomo:** Lida nativamente com *checkpoints*, novas tentativas (retries) e otimiza√ß√£o de performance.
* **Opera√ß√µes Avan√ßadas Simplificadas:** Suporte nativo e facilitado para CDC (Change Data Capture), SCD Tipo 2 e verifica√ß√µes de Qualidade de Dados (Expectations).
* **Valida√ß√£o:** Suporte a "Dry Run" para validar a l√≥gica do pipeline sem processar dados.

---

## üß± Tipos de Objetos no LDP

O LDP permite a defini√ß√£o de tr√™s tipos principais de objetos, tanto em Python quanto em SQL:

| Tipo de Objeto | Persist√™ncia | Comportamento de Processamento | Caso de Uso Ideal |
| --- | --- | --- | --- |
| **Streaming Table** | Sim (Cat√°logo) | **Incremental.** Processa apenas novos dados desde a √∫ltima execu√ß√£o (append-only). | Ingest√£o quase em tempo real (baixa lat√™ncia/ms), leitura de Kafka/Autoloader. |
| **Materialized View** | Sim (Cat√°logo) | **Atualiza√ß√£o Completa** (geralmente). Recarrega/substitui dados a cada execu√ß√£o.* | Agrega√ß√µes complexas, Joins, dados que sofrem updates/deletes na origem. Lat√™ncia de segundos/minutos. |
| **Temporary View** | N√£o (Ef√™mera) | Processamento tempor√°rio durante a execu√ß√£o do pipeline. | Transforma√ß√µes intermedi√°rias e verifica√ß√µes de qualidade que n√£o precisam ser salvas. |

**Nota: Em computa√ß√£o Serverless, algumas Materialized Views podem ser otimizadas para atualiza√ß√µes incrementais.*

---

## ‚ö° LDP vs. Apache Spark Tradicional

### Sintaxe Python

* **Spark:** Exige defini√ß√£o expl√≠cita de `readStream`, `writeStream` e gest√£o manual de caminhos de checkpoint.
* **LDP:** Utiliza **decoradores** (`@table` ou `@dlt.table`). A escrita e o checkpoint s√£o abstra√≠dos pelo framework.

### Sintaxe SQL

* **Spark:** N√£o suporta nativamente a cria√ß√£o de pipelines de streaming apenas com SQL (exige API PySpark).
* **LDP:** Suporte nativo total via SQL:
```sql
CREATE OR REFRESH STREAMING TABLE nome_tabela AS SELECT ...

```



---

## üõ†Ô∏è Mudan√ßas da Estrutura Antiga (DLT)

* **Arquivos:** Migra√ß√£o de base exclusiva em Notebooks para suporte a arquivos de script (`.py`, `.sql`).
* **Valida√ß√£o:** Introdu√ß√£o do conceito de execu√ß√£o de teste ("Dry Run") para valida√ß√£o de c√≥digo.
* **Sintaxe:** Evolu√ß√£o dos decoradores e comandos para maior clareza entre objetos de streaming e est√°ticos.

---

### Pr√≥ximo passo

Voc√™ gostaria de ver um exemplo pr√°tico de c√≥digo convertendo um pipeline **PySpark padr√£o** para a sintaxe **LDP (Python)** para visualizar as diferen√ßas?