# Lake Flow Declarative Pipelines (LDP)

## üìñ Vis√£o Geral

O **Lake Flow Declarative Pipelines (LDP)** (anteriormente conhecido como *Delta Live Tables* ou DLT) √© uma estrutura de ETL baseada no Apache Spark projetada para criar pipelines de dados confi√°veis e de f√°cil manuten√ß√£o.

![alt text](image-6.png)
![alt text](image-4.png)

Diferente da abordagem imperativa padr√£o do Spark, o LDP utiliza um modelo **declarativo**: voc√™ define o resultado desejado das transforma√ß√µes (o "o qu√™") e o framework gerencia a complexidade da execu√ß√£o, orquestra√ß√£o e infraestrutura (o "como").

> **Nota:** A Databricks abriu o c√≥digo-fonte desta solu√ß√£o, integrando-a ao ecossistema Apache Spark sob o nome **Spark Declarative Pipelines**.

![alt text](image.png)

---

## üöÄ Principais Benef√≠cios

* **Abstra√ß√£o de Complexidade:** Elimina a necessidade de gerenciar detalhes de baixo n√≠vel do Spark(checkpoints, escritas de stream).
* **Orquestra√ß√£o Autom√°tica:** Identifica depend√™ncias entre tabelas e visualiza o fluxo de execu√ß√£o (DAG) automaticamente.
* **Gerenciamento Aut√¥nomo:** Lida nativamente com *checkpoints*, novas tentativas (retries) e otimiza√ß√£o de performance.
* **Opera√ß√µes Avan√ßadas Simplificadas:** Suporte nativo e facilitado para CDC (Change Data Capture), SCD Tipo 2 e verifica√ß√µes de Qualidade de Dados (Expectations).
* **Valida√ß√£o:** Suporte a "Dry Run" para validar a l√≥gica do pipeline sem processar dados.

![alt text](image-1.png)
![alt text](image-2.png)
![alt text](image-3.png)
---

## üß± Tipos de Objetos no LDP

O LDP permite a defini√ß√£o de tr√™s tipos principais de objetos, tanto em Python quanto em SQL:

| Tipo de Objeto | Persist√™ncia | Comportamento de Processamento | Caso de Uso Ideal |
| --- | --- | --- | --- |
| **Streaming Table** | Sim (Cat√°logo) | **Incremental.** Processa apenas novos dados desde a √∫ltima execu√ß√£o (append-only). | Ingest√£o quase em tempo real (baixa lat√™ncia/ms), leitura de Kafka/Autoloader. |
| **Materialized View** | Sim (Cat√°logo) | **Atualiza√ß√£o Completa** (geralmente). Recarrega/substitui dados a cada execu√ß√£o.* | Agrega√ß√µes complexas, Joins, dados que sofrem updates/deletes na origem, source n√£o streaming, boa para BI complexo. N√£o possui design para low-latency caso de uso a lat√™ncia de atualizac√£o est√° em minutos ou segundos. |
| **Temporary View** | N√£o (Ef√™mera) | Processamento tempor√°rio durante a execu√ß√£o do pipeline. | Transforma√ß√µes intermedi√°rias e verifica√ß√µes de qualidade que n√£o precisam ser salvas. |

**Nota: Em computa√ß√£o Serverless, algumas Materialized Views podem ser otimizadas para atualiza√ß√µes incrementais.*
---

## ‚ö° LDP vs. Apache Spark Tradicional

### Sintaxe Python

* **Spark:** Exige defini√ß√£o expl√≠cita de `readStream`, `writeStream` e gest√£o manual de caminhos de checkpoint.
* **LDP:** Utiliza **decoradores** (`@table` ou `@dlt.table`). A escrita e o checkpoint s√£o abstra√≠dos pelo framework.

### Sintaxe SQL

* **Spark:** N√£o suporta nativamente a cria√ß√£o de pipelines de streaming apenas com SQL (exige API PySpark).
* **LDP:** Suporte nativo total via SQL:
```sql
CREATE OR REFRESH STREAMING TABLE nome_tabela AS SELECT ...
```
---

## üõ†Ô∏è Mudan√ßas da Estrutura Antiga (DLT)

* **Arquivos:** Migra√ß√£o de base exclusiva em Notebooks para suporte a arquivos de script (`.py`, `.sql`).
* **Valida√ß√£o:** Introdu√ß√£o do conceito de execu√ß√£o de teste ("Dry Run") para valida√ß√£o de c√≥digo. O DLT era baseado em notebooks e tinhamos a op√ß√£o de validar o c√≥digo-fonte usando o bot√£o de valida√ß√£o no DLT. Por outro lado agora se baseia em arquivos de script com extens√£o .py, SQL e o dry-run ajuda a realizar a valida√ß√£o seca para o c√≥digo-fonte do nosso pipeline sem atualizar nenhum dado.

![alt text](image-5.png)

* **Sintaxe:** Evolu√ß√£o dos decoradores e comandos para maior clareza entre objetos de streaming e est√°ticos.

---

# Projeto Bookstore: Ingest√£o com LDP (Lake Flow Declarative Pipelines)

## üéØ Objetivo do M√≥dulo

Refatorar o c√≥digo PySpark original do projeto "Bookstore" para o estilo declarativo do **LDP**. O foco desta etapa √© a **Ingest√£o de Dados**, criando:

1. **Tabela Bronze (Multiplexer):** Como uma *Streaming Table* (ingest√£o incremental).
2. **Country Lookup:** Como uma *Temporary View* (auxiliar para joins futuros).

---

## ‚öôÔ∏è Configura√ß√£o do Pipeline (UI)

### Cria√ß√£o e Estrutura

* **Caminho:** `Jobs & Pipelines` > `Create ETL Pipeline`.
* **Nome:** `Bookstore Pro Pipeline`.
* **Cat√°logo/Schema:** `professional`.

![alt text](image-7.png)
![alt text](image-8.png)

Permite visualizar tabelas e performance:

![alt text](image-9.png)

### Organiza√ß√£o de Arquivos

O LDP sugere uma estrutura de pastas para organizar o c√≥digo:

* **`transformations/`**: Pasta padr√£o para os scripts do pipeline (c√≥digo-fonte ativo).
* **`explorations/`**: Pasta sugerida para Notebooks auxiliares (ex: an√°lise de dados) que **n√£o** s√£o executados como parte do fluxo do pipeline.

![alt text](image-10.png)
![alt text](image-11.png)

Voc√™ pode incluir os arquivos do exploration no pipeline source code:

![alt text](image-12.png)

### Configura√ß√µes de Execu√ß√£o (Settings)

* **Modo do Pipeline:**
* *Triggered (Acionado):* Processa os dados dispon√≠veis e desliga (Batch/Incremental). **Selecionado para esta demo**.
* *Continuous:* Executa continuamente (ex: a cada 2 segundos) para lat√™ncia m√≠nima.

Settings -> Pipeline Mode
![alt text](image-13.png)

* **Par√¢metros:** Adi√ß√£o da chave `dataset_path` para definir o diret√≥rio de origem dos dados. (Settings -> Configuration)

![alt text](image-14.png)

* **Event Logs:** Configura√ß√£o (em *Advanced Settings*) de uma tabela para armazenar logs de execu√ß√£o do pipeline.

![alt text](image-15.png)

---

## üíª Implementa√ß√£o (Python/LDP)

### 1. Setup Inicial

Importa√ß√£o do m√≥dulo `dlt` (ou `pipelines`) e leitura de par√¢metros configurados na UI.

```python
import dlt
from pyspark.sql.functions import *

# Ler configura√ß√£o definida na UI do Pipeline
dataset_path = spark.conf.get("dataset_path")

```

### 2. Tabela Bronze (Streaming Table)

Utiliza o Auto Loader (`cloudFiles`) para ingest√£o incremental.

* **Decorador:** `@dlt.table` (gerencia escritas e checkpoints).
* **Propriedades Cr√≠ticas:**
* `delta.appendOnly = true`: Otimiza para tabelas que s√≥ recebem inser√ß√µes, desabilitando updates e deletes para a tabela bronze.
* `pipelines.reset.allowed = false`: **Prote√ß√£o de Dados.** Impede que esta tabela seja apagada/zerada se for solicitado um "Full Refresh" do pipeline (essencial para camada Bronze onde a reten√ß√£o da fonte pode ser curta).

![alt text](image-16.png)

```python
@dp.table(
    name = "bronze", #Par√¢metro Name
    partition_cols=["topic", "year_month"],
    table_properties={
        "delta.appendOnly": "true",
        "pipelines.reset.allowed": "false",
    }
)

# Por default, o nome da tabela ser√° o mesmo que o nome da fun√ß√£o, mas, se preferir, pode-se adicionar o par√™metro name para especificar o nome da tabela diferente.
# A tabela ser√° criada no cat√°logo e no schema padr√£o que voc√™ definir na configura√ß√£o do pipeline, mas voc√™ pode usar o namespace de tr√™s n√≠veis para manter o objeto em um cat√°logo ou esquema diferente.
def process_bronze():
    schema = "key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG"

    bronze_df = (spark.readStream
                    .format("cloudFiles")
                    .option("cloudFiles.format", "json")
                    .schema(schema)
                    .load(f"{dataset_path}/kafka-raw-etl")
                    .withColumn("timestamp", (F.col("timestamp")/1000).cast("timestamp"))  
                    .withColumn("year_month", F.date_format("timestamp", "yyyy-MM"))
                )

    return bronze_df

```

### 3. Visualiza√ß√£o Tempor√°ria (Temporary View)

Usada para dados auxiliares que n√£o precisam ser persistidos no Data Lake, apenas usados durante a execu√ß√£o.

```python
@dp.temporary_view
def country_lookup():
    countries_df = spark.read.json(f"{dataset_path}/country_lookup")
    return countries_df
```

---

## üöÄ Modos de Execu√ß√£o e Ciclo de Vida

### Valida√ß√£o

* **Dry Run (Execu√ß√£o Seca):** Valida a sintaxe e a l√≥gica do c√≥digo sem processar dados ou criar tabelas.

### Execu√ß√£o (Run Pipeline)

* **Development Mode (Modo Desenvolvimento):**
* Reutiliza o mesmo cluster (mant√©m ativo por ~2 horas) para evitar tempo de reinicializa√ß√£o.
* Desativa novas tentativas autom√°ticas (retries) para falhar r√°pido e permitir debug.


* **Production Mode (Modo Produ√ß√£o):**
* Cria um novo cluster para cada execu√ß√£o (e encerra ao fim).
* Habilita *retries* autom√°ticos para falhas transit√≥rias (ex: erro de inicializa√ß√£o, vazamento de mem√≥ria).

Se quiser alternar de um modo para outro:
![alt text](image-17.png)
![alt text](image-18.png)
 
---

# Qualidade de Dados no LDP: Expectativas (Expectations)

## üõ°Ô∏è O que s√£o Expectativas?

As **Expectativas** s√£o restri√ß√µes (constraints) de validade aplicadas aos dados √† medida que fluem pelo pipeline ETL. Elas funcionam como "Unit Tests" para dados, garantindo integridade e confiabilidade.

![alt text](image-20.png)

* Definem condi√ß√µes booleanas que cada registro deve atender.
* Todas as viola√ß√µes s√£o rastreadas e relatadas nas **m√©tricas** do pipeline.

![alt text](image-21.png)

* Podem ser aplicadas em tabelas e views usando SQL ou Python.
* Por default registros que violam alguma constrains s√£o mantidas na tabela.

![alt text](image-22.png)

## üö¶ A√ß√µes de Viola√ß√£o (Policy)

Voc√™ pode definir o que acontece quando um registro falha na valida√ß√£o. Existem tr√™s n√≠veis de severidade:

![alt text](image-19.png)

---

## üíª Sintaxe: Python vs. SQL

### Sintaxe B√°sica (Uma restri√ß√£o)

| A√ß√£o | Fun√ß√£o Python (`@dlt`) | Sintaxe SQL (`CONSTRAINT`) |
| --- | --- | --- |
| **Warn** | `@dlt.expect("descri√ß√£o", "condi√ß√£o")` | `CONSTRAINT nome EXPECT (condi√ß√£o)` |
| **Drop** | `@dlt.expect_or_drop("descri√ß√£o", "condi√ß√£o")` | `CONSTRAINT nome EXPECT (condi√ß√£o) ON VIOLATION DROP ROW` |
| **Fail** | `@dlt.expect_or_fail("descri√ß√£o", "condi√ß√£o")` | `CONSTRAINT nome EXPECT (condi√ß√£o) ON VIOLATION FAIL UPDATE` |

![alt text](image-24.png)

### 1. `expect_or_drop` (Individual)

Define **uma √∫nica** regra de valida√ß√£o.

* **Como funciona:** Voc√™ passa o nome da regra e a condi√ß√£o booleana.
* **Quando usar:** Quando voc√™ quer aplicar apenas uma restri√ß√£o espec√≠fica ou prefere escrever cada regra em uma linha separada no seu c√≥digo.
* **Exemplo:**
```python
@dp.expect_or_drop("id_valido", "id IS NOT NULL")

```

### 2. `expect_all_or_drop` (Coletivo)

Define **m√∫ltiplas** regras de valida√ß√£o simultaneamente usando um dicion√°rio Python.

* **Como funciona:** Voc√™ passa um dicion√°rio onde as chaves s√£o os nomes das regras e os valores s√£o as condi√ß√µes.
* **Comportamento:** Se o registro violar **qualquer uma** das regras listadas no dicion√°rio, ele ser√° descartado. Funciona como um operador `AND` l√≥gico para a validade (todas devem ser verdadeiras para o registro passar).
* **Quando usar:** Para deixar o c√≥digo mais limpo quando se tem muitas valida√ß√µes para a mesma tabela.
* **Exemplo:**
```python
@dp.expect_all_or_drop({
    "id_valido": "id IS NOT NULL",
    "valor_positivo": "valor > 0",
    "data_recente": "data >= '2023-01-01'"
})

```



### Resumo

| Fun√ß√£o | Quantidade de Regras | Entrada (Input) |
| --- | --- | --- |
| **expect_or_drop** | 1 Regra | Dois argumentos: `(nome, condi√ß√£o)` |
| **expect_all_or_drop** | V√°rias Regras | Um argumento: Dicion√°rio `{'nome': 'condi√ß√£o', ...}` |

Em ambos os casos, a **a√ß√£o** √© a mesma: se a condi√ß√£o n√£o for atendida, a linha (row) √© removida do dataset final e a viola√ß√£o √© registrada nas m√©tricas.

### Expectativas M√∫ltiplas (Apenas Python)

No Python, √© poss√≠vel passar um **dicion√°rio** de regras para aplicar v√°rias valida√ß√µes de uma s√≥ vez, reduzindo a verbosidade do c√≥digo.

* `@dlt.expect_all({"regra1": "condicao1", "regra2": "condicao2"})`
* `@dlt.expect_all_or_drop({...})`
* `@dlt.expect_all_or_fail({...})`

---

## üìù Exemplo Pr√°tico

![alt text](image-23.png)

Aqui est√° o resumo estruturado para o m√≥dulo de **Qualidade de Dados na Camada Prata**, seguindo o padr√£o README.

---

# Projeto Bookstore: Qualidade de Dados (Camada Prata)

## üéØ Objetivo do M√≥dulo

Implementar a **Camada Prata** do pipeline, focando no refinamento dos dados brutos (Bronze) atrav√©s da aplica√ß√£o de **Expectativas (Data Quality Expectations)**.
O objetivo √© processar pedidos e livros, garantindo que apenas dados v√°lidos sejam utilizados ou analisados, aplicando diferentes estrat√©gias de tratamento de erro (Alertar, Falhar, Descartar ou Quarentenar).

---

## üõ†Ô∏è Configura√ß√£o e Fluxo de Trabalho

1. **Acesso:** `Jobs & Pipelines` > Selecionar `Bookstore Pro Pipeline`.
2. **Edi√ß√£o:** Clicar em `Edit Pipeline` para adicionar novos scripts.
3. **Fonte de Dados:** Leitura da tabela `bronze_multiplex` (definida anteriormente).
* *Nota:* Gra√ßas √† propriedade `pipelines.reset.allowed = false` definida na aula anterior, um "Full Refresh" na camada Prata **n√£o** apaga/reprocessa a ingest√£o da Bronze.

---

## üö¶ Estrat√©gias de Qualidade de Dados (Implementa√ß√£o)

### 1. Apenas Alertar (Monitoramento)

Permite a passagem de dados inv√°lidos, mas registra o erro nas m√©tricas.

* **Fun√ß√£o:** `@dlt.expect`
* **Cen√°rio:** Validar se a quantidade do pedido √© maior que zero.

```python
@dp.table
@dp.expect("valid_quantity", "quantity > 0")
def orders_silver():
    orders_df = process_orders()
    return orders_df
```

* **Resultado:** Todos os registros s√£o gravados. Viola√ß√µes aparecem como "Warnings" na aba *Table Metrics*.

### 2. Interromper o Pipeline (Fail)

Garante integridade estrita. Se um √∫nico registro falhar, o pipeline para.

* **Fun√ß√£o:** `@dlt.expect_or_fail`
* **Resultado:** O pipeline falha e reverte a transa√ß√£o. Exige corre√ß√£o na fonte ou ajuste na regra.

### 3. Descartar Registros Inv√°lidos (Drop)

O pipeline continua, mas "filtra" silenciosamente os dados ruins antes da grava√ß√£o.

* **Fun√ß√£o:** `@dlt.expect_or_drop`
* **Resultado:** A tabela final cont√©m apenas registros v√°lidos (ex: 976 de 1000). As m√©tricas mostram quantos foram descartados.

```python
@dp.table
@dp.expect_or_drop("valid_quantity", "quantity <= 0")
def orders_quarantine():
    orders_df = process_orders()
    return orders_df
```

---

### 5. Sinaliza√ß√£o (Flagging / Is Quarantined)

Em vez de separar tabelas, adiciona-se uma coluna booleana indicando a validade do registro. √ötil para particionamento.

* **L√≥gica:** Usar `expect_all` para m√©tricas, mas criar uma coluna na transforma√ß√£o.

```python
# Exemplo conceitual
rules = {"valid_price": "price > 0", "valid_date": "updated_at IS NOT NULL"}

@dlt.view(temporary=True)
@dlt.expect_all(rules)
def books_silver():
    return (
        read_bronze_books()
        .withColumn("is_quarantined", expr("NOT(price > 0 AND updated_at IS NOT NULL)"))
    )

```

---

## üìä Monitoramento e Observabilidade

### Via Interface (UI)

* **Graph:** Clicar na tabela > Aba **Table Metrics**.
* Mostra contagem de registros escritos vs. registros descartados/falhos por regra.

### Via Program√°tica (Event Log)

Os resultados de qualidade s√£o gravados na tabela de logs configurada no pipeline (`event_log`). √â poss√≠vel consultar via SQL para relat√≥rios automatizados.

**Exemplo de Query no Event Log:**
Ocorre em eventos onde `event_type = 'flow_progress'`.

```sql
SELECT
  details:flow_progress:data_quality:expectations
FROM event_log_table
WHERE event_type = 'flow_progress'
```

Isso retorna um JSON contendo o nome da expectativa, dataset associado e contagem de pass/fail.

Aqui est√° o resumo estruturado sobre **APIs de CDC Autom√°tico** no Lake Flow, formatado como um README t√©cnico.

---

# Lake Flow: APIs de CDC Autom√°tico (Auto CDC)

## üìñ Vis√£o Geral

As **APIs de CDC Autom√°tico** s√£o a nova abordagem simplificada dos Pipelines Declarativos (LDP) para processar fluxos de dados de *Change Data Capture* (CDC).
Elas substituem a complexidade manual de escrever instru√ß√µes `MERGE INTO`, abstraindo a l√≥gica dif√≠cil de sequenciamento e versionamento.

### Principais Vantagens vs. Abordagem Tradicional (`MERGE`)

* **Gest√£o de Sequ√™ncia:** Trata automaticamente registros que chegam fora de ordem (*out-of-sequence records*). O `MERGE` tradicional exigia l√≥gica complexa para evitar sobrescrever dados novos com dados antigos.
* **Simplicidade:** Elimina centenas de linhas de c√≥digo manual.
* **Suporte a SCD:** Suporte nativo para **SCD Tipo 1** (Atualiza√ß√£o/Sobrescrita) e **SCD Tipo 2** (Hist√≥rico de altera√ß√µes).

---

## ‚öôÔ∏è Como Funciona (Arquitetura Interna)

Quando voc√™ declara uma tabela usando Auto CDC, o LDP cria dois objetos sob o cap√¥ para gerenciar a consist√™ncia:

1. **Tabela Interna (Storage):** Mant√©m o controle de todos os eventos de mudan√ßa.
* Utiliza "marcadores de l√°pide" (*tombstone markers*) para sinalizar linhas exclu√≠das (garantindo que exclus√µes n√£o sejam efetivadas at√© que sequ√™ncias atrasadas sejam processadas).
* Armazena metadados para reordenar dados fora de sequ√™ncia.

2. **View de Destino:** Uma visualiza√ß√£o com o nome da tabela alvo que apresenta apenas o **estado limpo e mais recente** dos dados para o usu√°rio final.

---

## üíª Sintaxe e Par√¢metros

### Sintaxe SQL

O comando base √© `CREATE STREAM ... AS AUTO CDC`.
*Nota: A tabela de streaming de destino j√° deve ter sido criada antes da execu√ß√£o deste comando.*

![alt text](image-26.png)

### Par√¢metros Principais

![alt text](image-25.png)

### Sintaxe Python

Utiliza a fun√ß√£o `create_auto_cdc_stream`. Os par√¢metros l√≥gicos s√£o os mesmos do SQL (keys, sequence, etc.).

![alt text](image-27.png)

---

## ‚ö†Ô∏è Nota sobre Legado (Exames de Certifica√ß√£o)

* **APPLY CHANGES INTO:** √â a sintaxe antiga (do Delta Live Tables original) para fazer CDC.
* **Status Atual:** Ainda √© suportada para retrocompatibilidade e **pode cair no exame atual**, mas a Databricks recomenda fortemente o uso das novas **APIs de Auto CDC**.

![alt text](image-28.png)
![alt text](image-29.png)
---
Aqui est√° o resumo estruturado para o m√≥dulo final do pipeline "Bookstore", cobrindo **Auto CDC**, **Camada Dourada (Gold)** e **Mistura de Python/SQL**, seguindo o padr√£o README.

---

# Projeto Bookstore: Camadas Silver (CDC) e Gold

## üéØ Objetivo do M√≥dulo

Finalizar o pipeline implementando l√≥gicas avan√ßadas de processamento:

1. **Auto CDC Tipo 1 (Python):** Para atualizar dados de Clientes (*Upsert*) -> √â o default SCD 1.
2. **Auto CDC Tipo 2 (SQL):** Para versionar o hist√≥rico de Livros (*SCD Type 2*).
3. **Camada Dourada:** Cria√ß√£o de *Materialized Views* para agrega√ß√µes de neg√≥cios (Estat√≠sticas por Pa√≠s e Autor).
4. **Hibridismo:** Uso simult√¢neo de Python e SQL no mesmo pipeline.

---

## üîÑ Implementa√ß√£o do Auto CDC

### 1. Auto CDC Tipo 1: Clientes (Python)

Atualiza a tabela de clientes onde novos dados sobrescrevem os antigos (*Upsert*).

* **Fonte:** View tempor√°ria unindo dados de CDC da Bronze + Lookup de Pa√≠ses.
* **Destino:** Tabela `customers_silver`.
* **L√≥gica:**
* `keys=["customer_id"]`: Chave prim√°ria.
* `sequence_by="order_timestamp"`: Define qual registro √© o mais recente em caso de conflito.
* `apply_as_delete_when`: (Opcional) Define l√≥gica para dele√ß√µes.
* **Resultado:** 396 registros processados (inserts/updates).



### 2. Auto CDC Tipo 2: Livros (SQL)

Mant√©m hist√≥rico de altera√ß√µes de pre√ßo ou detalhes do livro, criando novas linhas para cada vers√£o (*SCD Type 2*).

* **Sintaxe SQL:**
```sql
CREATE STREAMING TABLE books_silver;

APPLY CHANGES INTO books_silver
FROM STREAM(books_bronze_view)
KEYS (book_id)
SEQUENCE BY updated_at
COLUMNS * EXCEPT (updated_at)
STORED AS SCD TYPE 2; -- Habilita o hist√≥rico

```


* **Resultado:**
* Adiciona colunas autom√°ticas `__start_at` e `__end_at`.
* O registro atual possui `__end_at` como `NULL`.
* Tabelas SCD Type 2 **n√£o podem ser lidas como stream** diretamente por views materializadas downstream (requer leitura est√°tica ou filtros).



---

## üèÜ Camada Dourada (Materialized Views)

As Views Materializadas s√£o usadas aqui porque requerem agrega√ß√µes complexas e joins em dados que podem sofrer updates (n√£o-append-only).

### 1. Estat√≠sticas Di√°rias por Pa√≠s (Python)

* **Tipo:** `Materialized View`.
* **Fonte:** Join entre `orders` e `customers`.
* **Importante:** Usa `spark.table("...")` (leitura est√°tica) em vez de `readStream`, pois a tabela de clientes sofre updates (CDC) e n√£o suporta leitura de streaming padr√£o.

### 2. Vendas por Autor com Janelas (SQL)

* **Objetivo:** Agrega√ß√£o complexa com janelas de tempo.
* **T√©cnica:** Window Function com deslocamento (*offset*).
* `WINDOW(order_timestamp, '5 minutes', '5 minutes', '2 minutes')`
* Cria janelas de 5 min, sem sobreposi√ß√£o, deslocadas em 2 min.

---

## üõ°Ô∏è Qualidade de Dados em SQL

Demonstra√ß√£o de como aplicar *Constraints* diretamente no SQL durante a cria√ß√£o de tabelas da camada Silver/Gold.

```sql
CONSTRAINT valid_id EXPECT (id IS NOT NULL) ON VIOLATION DROP ROW,
CONSTRAINT valid_amount EXPECT (amount > 0) ON VIOLATION FAIL UPDATE

```

---

## ‚ö° Otimiza√ß√£o e Computa√ß√£o Serverless

* **Atualiza√ß√£o Incremental em Materialized Views:**
* Ao usar **Serverless Compute**, o Databricks tenta atualizar Views Materializadas de forma incremental (processando apenas o delta) mesmo para agrega√ß√µes.
* Identificado na UI pelo r√≥tulo **"Incremental"**.
* Se a l√≥gica for muito complexa ou usar Compute Cl√°ssico, o sistema reverte automaticamente para **Full Refresh** (rec√°lculo total).

---

Aqui est√° o resumo estruturado sobre o **Agendamento e Orquestra√ß√£o Avan√ßada de Jobs no Databricks**, formatado como um README t√©cnico.

---

# Databricks Workflows: Orquestra√ß√£o de Pipeline ETL Avan√ßada

## üéØ Objetivo

Criar um fluxo de trabalho (Job) robusto chamado **Bookstore Pro v2** que automatiza o ciclo de vida do pipeline ETL, incluindo ingest√£o condicional, execu√ß√£o do pipeline (DLT) e an√°lise din√¢mica de tabelas p√≥s-execu√ß√£o.

---

## üèóÔ∏è Estrutura do Job e Tarefas

### 1. Ingest√£o de Dados (`land_new_data`)

* **Tipo:** Notebook Task.
* **Fun√ß√£o:** Simula a chegada de novos arquivos na pasta de origem.
* **Recurso Chave:** Utiliza `dbutils.jobs.taskValues.set()` para passar a contagem de arquivos novos para a pr√≥xima tarefa.
* *Exemplo:* `key="number_new_files", value=count`.



### 2. Condicional (`has_new_file`)

* **Tipo:** If/Else Condition Task.
* **L√≥gica:** Verifica se h√° dados novos para processar.
* **Condi√ß√£o:** `tasks.land_new_data.values.number_new_files > 0`.
* **Ramifica√ß√µes:**
* **True:** Segue para executar o Pipeline ETL.
* **False:** Segue para tarefa de Log (n√£o h√° dados).

### 3. Execu√ß√£o do Pipeline (`run_pipeline`) - *Ramo True*

* **Tipo:** Delta Live Tables Pipeline Task.
* **A√ß√£o:** Aciona a atualiza√ß√£o (Triggered Update) do pipeline `Bookstore Pro Pipeline`.

### 4. Logging (`log_no_data`) - *Ramo False*

* **Tipo:** Notebook Task.
* **Fun√ß√£o:** Registra que o job rodou mas n√£o havia dados.
* **Par√¢metros Din√¢micos:** Usa vari√°veis de contexto do job para enriquecer o log.
* `{{job.id}}`: ID do Job.
* `{{job.start_time}}`: Data/hora de in√≠cio (ISO Format, UTC).


* **Notifica√ß√£o:** Configurada para enviar e-mail em caso de sucesso (aviso proativo).

### 5. Listagem de Tabelas (`list_tables`)

* **Tipo:** Notebook Task (executa ap√≥s o Pipeline).
* **Fun√ß√£o:** Consulta o `system.information_schema` para listar dinamicamente todas as tabelas e views criadas pelo pipeline.
* **Sa√≠da:** Retorna um Array de objetos (JSON) contendo nome e tipo das tabelas via `taskValues`.

### 6. An√°lise Din√¢mica (`tables_iterator`)

* **Tipo:** For Each Loop Task.
* **Input:** Array retornado pela tarefa `list_tables`.
* **Concorr√™ncia:** Configur√°vel (permite analisar v√°rias tabelas em paralelo).
* **Tarefa Aninhada (`analyze_table`):**
* Notebook que roda estat√≠sticas simples (count, sample) para cada tabela.
* **Par√¢metros:** Recebe valores din√¢micos do iterador: `{{input.table_name}}`, `{{input.table_type}}`.

---

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas e Dicas

### Par√¢metros de Trabalho (Job Parameters)

* Definidos no n√≠vel do Job (menu direito), s√£o propagados automaticamente para todas as tarefas.
* √ötil para constantes globais como `catalog` e `schema`.

### Reparo de Execu√ß√£o (Repair Run)

* Funcionalidade cr√≠tica para falhas.
* Se um job falha (ex: par√¢metro esquecido), permite:
1. Corrigir o erro ou injetar o par√¢metro faltante na hora.
2. Reiniciar o job **apenas a partir das tarefas que falharam ou foram puladas**, sem reexecutar o que j√° teve sucesso.

### Vari√°veis Din√¢micas

O Databricks oferece uma sintaxe de "chaves duplas" `{{...}}` para injetar contexto em tempo de execu√ß√£o:

* **Valores de Tarefas:** `{{tasks.<task_name>.values.<key>}}`
* **Tempo:** `{{job.start_time}}` (UTC)
* **Loop:** `{{input.<field_name>}}`

---